"""
BGE-M3æ–‡æœ¬åµŒå…¥æ¨¡å‹æœåŠ¡
æä¾›ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–åŠŸèƒ½
"""
import asyncio
import os
import time
from typing import List, Dict, Any, Optional, Union

# é…ç½®æ—¥å¿—ç¯å¢ƒå˜é‡ï¼ŒæŠ‘åˆ¶C++åº“çš„æ—¥å¿—è­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # TensorFlowæ—¥å¿—çº§åˆ«
os.environ['GRPC_VERBOSITY'] = 'ERROR'    # gRPCæ—¥å¿—çº§åˆ«
os.environ['GLOG_minloglevel'] = '2'      # Googleæ—¥å¿—æœ€å°çº§åˆ«
os.environ['GLOG_v'] = '0'               # Googleæ—¥å¿—è¯¦ç»†ç¨‹åº¦
os.environ['PYTHONWARNINGS'] = 'ignore'  # å¿½ç•¥Pythonè­¦å‘Š

import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from app.services.ai_model_base import BaseAIModel, ModelType, ProviderType, ModelStatus, AIModelException
from app.utils.enum_helpers import get_enum_value
from app.core.config import get_settings
from app.core.logging_config import logger
settings = get_settings()


class BGEEmbeddingService(BaseAIModel):
    """
    BGE-M3æ–‡æœ¬åµŒå…¥æœåŠ¡

    BGE-M3æ˜¯BAAIå¼€å‘çš„å¤šè¯­è¨€ã€å¤šç²’åº¦ã€å¤šåŠŸèƒ½åµŒå…¥æ¨¡å‹
    ç‰¹åˆ«é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–ï¼Œæ”¯æŒ1024ç»´å‘é‡è¾“å‡º
    """

    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–BGE-M3åµŒå…¥æœåŠ¡

        Args:
            config: æ¨¡å‹é…ç½®å‚æ•°
        """
        default_config = {
            "model_name": "BAAI/bge-m3",
            "device": "cpu",
            "embedding_dim": 1024,
            "max_length": 8192,
            "normalize_embeddings": True,
            "batch_size": 32,
            "pooling_strategy": "cls",
            "use_sentence_transformers": False,  # é»˜è®¤ä½¿ç”¨Transformersè€Œä¸æ˜¯SentenceTransformers
            "cache_dir": None,
            "trust_remote_code": True
        }

        if config:
            default_config.update(config)

        super().__init__(
            model_name=default_config["model_name"],
            model_type=ModelType.EMBEDDING,
            provider=ProviderType.LOCAL,
            config=default_config
        )

        # æ¨¡å‹ç›¸å…³å±æ€§
        self.tokenizer = None
        self.model = None
        self.sentence_transformer = None
        self.device = self._setup_device()

        logger.info(f"åˆå§‹åŒ–BGE-M3åµŒå…¥æœåŠ¡ï¼Œè®¾å¤‡: {self.device}")

    def _setup_device(self) -> torch.device:
        """
        è®¾ç½®è®¡ç®—è®¾å¤‡

        Returns:
            torch.device: è®¡ç®—è®¾å¤‡
        """
        device_str = self.config.get("device", "cpu")
        if device_str == "auto":
            device_str = "cuda" if torch.cuda.is_available() else "cpu"

        device = torch.device(device_str)

        if device_str == "cuda":
            logger.info(f"ä½¿ç”¨GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        else:
            logger.info("ä½¿ç”¨CPUè®¾å¤‡")

        return device

    async def load_model(self) -> bool:
        """
        åŠ è½½BGE-M3æ¨¡å‹

        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            self.update_status(ModelStatus.LOADING)
            logger.info(f"å¼€å§‹åŠ è½½BGE-M3æ¨¡å‹: {self.model_name}")

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ¨¡å‹åŠ è½½ï¼ˆCPUå¯†é›†å‹æ“ä½œï¼‰
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._load_model_sync)

            self.update_status(ModelStatus.LOADED)
            logger.info(f"BGE-M3æ¨¡å‹åŠ è½½æˆåŠŸï¼ŒåµŒå…¥ç»´åº¦: {self.config['embedding_dim']}")
            return True

        except Exception as e:
            error_msg = f"BGE-M3æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            self.update_status(ModelStatus.ERROR, error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)

    def _load_model_sync(self):
        """åŒæ­¥åŠ è½½æ¨¡å‹"""
        model_name = self.config["model_name"]
        model_path = self.config.get("model_path")
        cache_dir = self.config.get("cache_dir")

        logger.info(f"BGEé…ç½®: model_name={model_name}, model_path={model_path}, use_sentence_transformers={self.config.get('use_sentence_transformers')}, cache_dir={cache_dir}")

        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„
        import os
        if model_path and os.path.exists(model_path):
            logger.info(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_path}")
            model_name = model_path
            use_sentence_transformers = False
            cache_dir = None  # æœ¬åœ°è·¯å¾„ä¸éœ€è¦ç¼“å­˜
        elif model_path:
            logger.warning(f"âš ï¸ æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨ç½‘ç»œä¸‹è½½: {model_path}")
            use_sentence_transformers = self.config.get("use_sentence_transformers", True)
        else:
            logger.info(f"ğŸŒ ä½¿ç”¨ç½‘ç»œæ¨¡å‹: {model_name}")
            use_sentence_transformers = self.config.get("use_sentence_transformers", True)

        # æ ¹æ®é…ç½®é€‰æ‹©åŠ è½½æ–¹å¼
        if use_sentence_transformers:
            logger.info("ä½¿ç”¨SentenceTransformersåŠ è½½BGE-M3æ¨¡å‹")
            self.sentence_transformer = SentenceTransformer(
                model_name,
                device=self.device,
                cache_folder=cache_dir
            )
            # è®¾ç½®æ¨¡å‹é…ç½®
            if self.config.get("normalize_embeddings", True):
                self.sentence_transformer.normalize_embeddings = True
        else:
            logger.info("ä½¿ç”¨Transformersåº“æ‰‹åŠ¨åŠ è½½BGE-M3æ¨¡å‹")

            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                trust_remote_code=True
            )

            # åŠ è½½æ¨¡å‹
            self.model = AutoModel.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32
            )

            # ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
            self.model.to(self.device)
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    async def unload_model(self) -> bool:
        """
        å¸è½½BGE-M3æ¨¡å‹

        Returns:
            bool: å¸è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹å¸è½½BGE-M3æ¨¡å‹: {self.model_name}")

            # æ¸…ç†å†…å­˜
            if self.sentence_transformer:
                del self.sentence_transformer
                self.sentence_transformer = None

            if self.model:
                del self.model
                self.model = None

            if self.tokenizer:
                del self.tokenizer
                self.tokenizer = None

            # æ¸…ç†GPUå†…å­˜
            if self.device.type == "cuda":
                torch.cuda.empty_cache()

            self.update_status(ModelStatus.UNLOADED)
            logger.info("BGE-M3æ¨¡å‹å¸è½½æˆåŠŸ")
            return True

        except Exception as e:
            error_msg = f"BGE-M3æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return False

    async def predict(self, texts: Union[str, List[str]], **kwargs) -> np.ndarray:
        """
        æ–‡æœ¬åµŒå…¥é¢„æµ‹

        Args:
            texts: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            **kwargs: å…¶ä»–é¢„æµ‹å‚æ•°
                - batch_size: æ‰¹å¤„ç†å¤§å°
                - show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
                - normalize_embeddings: æ˜¯å¦å½’ä¸€åŒ–åµŒå…¥å‘é‡

        Returns:
            np.ndarray: æ–‡æœ¬åµŒå…¥å‘é‡æ•°ç»„

        Raises:
            AIModelException: é¢„æµ‹å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        if self.status != ModelStatus.LOADED:
            raise AIModelException(
                f"æ¨¡å‹æœªåŠ è½½ï¼Œå½“å‰çŠ¶æ€: {get_enum_value(self.status)}",
                model_name=self.model_name
            )

        try:
            # æ ‡å‡†åŒ–è¾“å…¥
            if isinstance(texts, str):
                texts = [texts]

            if not texts:
                raise AIModelException("è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º", model_name=self.model_name)

            # è·å–é¢„æµ‹å‚æ•°
            batch_size = kwargs.get("batch_size", self.config.get("batch_size", 32))
            show_progress = kwargs.get("show_progress", False)
            normalize_embeddings = kwargs.get("normalize_embeddings", self.config.get("normalize_embeddings", True))

            logger.info(f"å¼€å§‹æ–‡æœ¬åµŒå…¥é¢„æµ‹ï¼Œæ–‡æœ¬æ•°é‡: {len(texts)}ï¼Œæ‰¹å¤§å°: {batch_size}")

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåµŒå…¥è®¡ç®—
            loop = asyncio.get_event_loop()
            embeddings = await loop.run_in_executor(
                None, self._encode_texts_sync, texts, batch_size, show_progress, normalize_embeddings
            )

            self.record_usage()
            logger.info(f"æ–‡æœ¬åµŒå…¥é¢„æµ‹å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {embeddings.shape}")
            return embeddings

        except Exception as e:
            error_msg = f"æ–‡æœ¬åµŒå…¥é¢„æµ‹å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)

    def _encode_texts_sync(self, texts: List[str], batch_size: int, show_progress: bool, normalize_embeddings: bool) -> np.ndarray:
        """
        åŒæ­¥æ‰§è¡Œæ–‡æœ¬ç¼–ç 

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            normalize_embeddings: æ˜¯å¦å½’ä¸€åŒ–

        Returns:
            np.ndarray: åµŒå…¥å‘é‡æ•°ç»„
        """
        if self.sentence_transformer:
            # ä½¿ç”¨SentenceTransformersç¼–ç 
            embeddings = self.sentence_transformer.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=show_progress,
                normalize_embeddings=normalize_embeddings,
                convert_to_numpy=True
            )
        else:
            # æ‰‹åŠ¨ä½¿ç”¨Transformersç¼–ç 
            embeddings = self._encode_with_transformers(texts, batch_size, normalize_embeddings)

        # BGE-M3å®é™…è¾“å‡º1024ç»´å‘é‡ï¼Œæ›´æ–°éªŒè¯é€»è¾‘
        embedding_dim = embeddings.shape[1]
        if embedding_dim == 1024:
            logger.info(f"âœ… BGE-M3æ­£ç¡®ç”Ÿæˆ {embedding_dim} ç»´åµŒå…¥å‘é‡")
        elif embedding_dim == 768:
            logger.info(f"âœ… BGE-M3ç”Ÿæˆ {embedding_dim} ç»´åµŒå…¥å‘é‡ï¼ˆæ ‡å‡†æ¨¡å¼ï¼‰")
        else:
            logger.warning(f"âš ï¸ BGE-M3åµŒå…¥ç»´åº¦å¼‚å¸¸: {embedding_dim} (æœŸæœ›1024ç»´)")

        return embeddings

    def _encode_with_transformers(self, texts: List[str], batch_size: int, normalize_embeddings: bool) -> np.ndarray:
        """
        ä½¿ç”¨Transformersåº“è¿›è¡Œç¼–ç 

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            normalize_embeddings: æ˜¯å¦å½’ä¸€åŒ–

        Returns:
            np.ndarray: åµŒå…¥å‘é‡æ•°ç»„
        """
        all_embeddings = []

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]

            # tokenize
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=self.config.get("max_length", 8192),
                return_tensors="pt"
            )

            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)

                # è·å–CLSå‘é‡æˆ–å¹³å‡æ± åŒ–
                if self.config.get("pooling_strategy", "cls") == "cls":
                    embeddings = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]
                else:
                    # å¹³å‡æ± åŒ–
                    attention_mask = inputs["attention_mask"]
                    token_embeddings = outputs.last_hidden_state
                    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
                    embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

            # è½¬æ¢ä¸ºnumpy
            embeddings = embeddings.cpu().numpy()
            all_embeddings.append(embeddings)

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        final_embeddings = np.vstack(all_embeddings)

        # å½’ä¸€åŒ–
        if normalize_embeddings:
            final_embeddings = final_embeddings / np.linalg.norm(final_embeddings, axis=1, keepdims=True)

        return final_embeddings

    async def compute_similarity(self, query_embedding: np.ndarray, corpus_embeddings: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—æŸ¥è¯¢å‘é‡ä¸è¯­æ–™åº“å‘é‡çš„ç›¸ä¼¼åº¦

        Args:
            query_embedding: æŸ¥è¯¢å‘é‡ [1, embedding_dim] æˆ– [embedding_dim]
            corpus_embeddings: è¯­æ–™åº“å‘é‡ [n_docs, embedding_dim]

        Returns:
            np.ndarray: ç›¸ä¼¼åº¦åˆ†æ•° [n_docs]
        """
        try:
            # ç¡®ä¿query_embeddingæ˜¯2D
            if query_embedding.ndim == 1:
                query_embedding = query_embedding.reshape(1, -1)

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = np.dot(corpus_embeddings, query_embedding.T).flatten()

            return similarities

        except Exception as e:
            error_msg = f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)

    async def search_similar(self, query: str, corpus_texts: List[str], top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æœ¬

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            corpus_texts: è¯­æ–™åº“æ–‡æœ¬åˆ—è¡¨
            top_k: è¿”å›Top-Kç»“æœ

        Returns:
            List[Dict[str, Any]]: ç›¸ä¼¼æ–‡æœ¬åˆ—è¡¨ï¼ŒåŒ…å«æ–‡æœ¬å†…å®¹å’Œç›¸ä¼¼åº¦åˆ†æ•°
        """
        try:
            # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
            query_embedding = await self.predict(query)

            # ç¼–ç è¯­æ–™åº“æ–‡æœ¬
            corpus_embeddings = await self.predict(corpus_texts)

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = await self.compute_similarity(query_embedding, corpus_embeddings)

            # è·å–Top-Kç»“æœ
            top_indices = np.argsort(similarities)[::-1][:top_k]

            results = []
            for idx in top_indices:
                results.append({
                    "text": corpus_texts[idx],
                    "similarity": float(similarities[idx]),
                    "index": int(idx)
                })

            return results

        except Exception as e:
            error_msg = f"ç›¸ä¼¼æ–‡æœ¬æœç´¢å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)

    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯

        Returns:
            Dict[str, Any]: æ¨¡å‹ä¿¡æ¯
        """
        return {
            "model_name": self.model_name,
            "model_type": get_enum_value(self.model_type),
            "provider": get_enum_value(self.provider),
            "embedding_dim": self.config.get("embedding_dim", 1024),
            "max_length": self.config.get("max_length", 8192),
            "device": str(self.device),
            "normalize_embeddings": self.config.get("normalize_embeddings", True),
            "pooling_strategy": self.config.get("pooling_strategy", "cls"),
            "use_sentence_transformers": self.config.get("use_sentence_transformers", True),
            "batch_size": self.config.get("batch_size", 32)
        }

    def _get_test_input(self) -> str:
        """è·å–å¥åº·æ£€æŸ¥çš„æµ‹è¯•è¾“å…¥"""
        return "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"

    async def benchmark_performance(self, test_texts: List[str], num_runs: int = 3) -> Dict[str, Any]:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•

        Args:
            test_texts: æµ‹è¯•æ–‡æœ¬åˆ—è¡¨
            num_runs: è¿è¡Œæ¬¡æ•°

        Returns:
            Dict[str, Any]: æ€§èƒ½æµ‹è¯•ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹BGE-M3æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œæ–‡æœ¬æ•°é‡: {len(test_texts)}, è¿è¡Œæ¬¡æ•°: {num_runs}")

            times = []
            for run in range(num_runs):
                start_time = time.time()
                await self.predict(test_texts)
                end_time = time.time()
                times.append(end_time - start_time)
                logger.info(f"ç¬¬{run + 1}æ¬¡è¿è¡Œè€—æ—¶: {times[-1]:.3f}ç§’")

            avg_time = np.mean(times)
            throughput = len(test_texts) / avg_time

            results = {
                "model_name": self.model_name,
                "num_texts": len(test_texts),
                "num_runs": num_runs,
                "times": times,
                "avg_time": float(avg_time),
                "min_time": float(np.min(times)),
                "max_time": float(np.max(times)),
                "throughput": float(throughput),  # texts per second
                "device": str(self.device)
            }

            logger.info(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼Œå¹³å‡è€—æ—¶: {avg_time:.3f}ç§’ï¼Œååé‡: {throughput:.1f} texts/s")
            return results

        except Exception as e:
            error_msg = f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)


# åˆ›å»ºBGE-M3æœåŠ¡å®ä¾‹çš„å·¥å‚å‡½æ•°
def create_bge_service(config: Dict[str, Any] = None) -> BGEEmbeddingService:
    """
    åˆ›å»ºBGE-M3åµŒå…¥æœåŠ¡å®ä¾‹

    Args:
        config: æ¨¡å‹é…ç½®å‚æ•°

    Returns:
        BGEEmbeddingService: BGE-M3æœåŠ¡å®ä¾‹
    """
    return BGEEmbeddingService(config or {})