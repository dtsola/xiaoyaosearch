"""
CN-CLIPå›¾åƒç†è§£æ¨¡å‹æœåŠ¡
æä¾›ä¸­æ–‡å›¾åƒ-æ–‡æœ¬ç†è§£å’ŒåŒ¹é…åŠŸèƒ½
"""
import asyncio
import logging
import os
import tempfile
import time
from typing import Dict, Any, Optional, List, Union, Tuple, BinaryIO
from pathlib import Path
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image, ImageOps
from transformers import CLIPProcessor, CLIPModel, ChineseCLIPProcessor, ChineseCLIPModel
import requests
from io import BytesIO

from app.services.ai_model_base import BaseAIModel, ModelType, ProviderType, ModelStatus, AIModelException
from app.utils.enum_helpers import get_enum_value

logger = logging.getLogger(__name__)


class CLIPVisionService(BaseAIModel):
    """
    CN-CLIPå›¾åƒç†è§£æœåŠ¡

    æä¾›ä¸­æ–‡å›¾åƒ-æ–‡æœ¬ç†è§£å’ŒåŒ¹é…åŠŸèƒ½
    """

    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–CLIPå›¾åƒç†è§£æœåŠ¡

        Args:
            config: æ¨¡å‹é…ç½®å‚æ•°
        """
        default_config = {
            "model_name": "OFA-Sys/chinese-clip-vit-base-patch16",
            "device": "cpu",
            "max_image_size": 512,
            "supported_formats": [".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".webp"],
            "max_file_size": 10 * 1024 * 1024,  # 10MB
            "normalize_embeddings": True,
            "batch_size": 16,
            "use_chinese_clip": True
        }

        if config:
            default_config.update(config)

        super().__init__(
            model_name=default_config["model_name"],
            model_type=ModelType.VISION,
            provider=ProviderType.LOCAL,
            config=default_config
        )

        # æ¨¡å‹ç›¸å…³å±æ€§
        self.model = None
        self.processor = None
        self.device = self._setup_device()

        logger.info(f"åˆå§‹åŒ–CLIPå›¾åƒç†è§£æœåŠ¡ï¼Œæ¨¡å‹: {self.config['model_name']}")

    def _setup_device(self) -> torch.device:
        """
        è®¾ç½®è®¡ç®—è®¾å¤‡

        Returns:
            torch.device: è®¡ç®—è®¾å¤‡
        """
        device_str = self.config.get("device", "cpu")
        if device_str == "auto":
            device_str = "cuda" if torch.cuda.is_available() else "cpu"

        device = torch.device(device_str)

        if device_str == "cuda":
            logger.info(f"ä½¿ç”¨GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        else:
            logger.info("ä½¿ç”¨CPUè®¾å¤‡")

        return device

    async def load_model(self) -> bool:
        """
        åŠ è½½CLIPæ¨¡å‹

        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            self.update_status(ModelStatus.LOADING)
            logger.info(f"å¼€å§‹åŠ è½½CLIPæ¨¡å‹: {self.model_name}")

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ¨¡å‹åŠ è½½
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._load_model_sync)

            self.update_status(ModelStatus.LOADED)
            logger.info(f"CLIPæ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ¨¡å‹: {self.config['model_name']}")
            return True

        except Exception as e:
            error_msg = f"CLIPæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            self.update_status(ModelStatus.ERROR, error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)

    def _load_model_sync(self):
        """åŒæ­¥åŠ è½½æ¨¡å‹"""
        model_name = self.config["model_name"]
        model_path = self.config.get("model_path")

        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„
        if model_path and os.path.exists(model_path):
            logger.info(f"âœ… ä½¿ç”¨æœ¬åœ°CLIPæ¨¡å‹è·¯å¾„: {model_path}")
            model_name = model_path
        elif model_path:
            logger.warning(f"âš ï¸ æœ¬åœ°CLIPæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨ç½‘ç»œä¸‹è½½: {model_path}")
        else:
            logger.info(f"ğŸŒ ä½¿ç”¨ç½‘ç»œCLIPæ¨¡å‹: {model_name}")

        logger.info(f"åŠ è½½CLIPæ¨¡å‹ {model_name} åˆ° {self.device}")

        # åŠ è½½CLIPæ¨¡å‹å’Œå¤„ç†å™¨
        self.model = ChineseCLIPModel.from_pretrained(model_name, local_files_only=bool(model_path and os.path.exists(model_path)))
        self.processor = ChineseCLIPProcessor.from_pretrained(model_name, local_files_only=bool(model_path and os.path.exists(model_path)))

        # ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
        self.model.to(self.device)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        logger.info("CLIPæ¨¡å‹åŠ è½½å®Œæˆ")

    async def unload_model(self) -> bool:
        """
        å¸è½½CLIPæ¨¡å‹

        Returns:
            bool: å¸è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹å¸è½½CLIPæ¨¡å‹: {self.model_name}")

            # æ¸…ç†æ¨¡å‹
            if self.model:
                del self.model
                self.model = None

            if self.processor:
                del self.processor
                self.processor = None

            # æ¸…ç†GPUå†…å­˜
            if self.device.type == "cuda":
                torch.cuda.empty_cache()

            self.update_status(ModelStatus.UNLOADED)
            logger.info("CLIPæ¨¡å‹å¸è½½æˆåŠŸ")
            return True

        except Exception as e:
            error_msg = f"CLIPæ¨¡å‹å¸è½½å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return False

    async def predict(self, inputs: Union[str, bytes, BinaryIO, Image.Image], texts: List[str], **kwargs) -> Dict[str, Any]:
        """
        å›¾åƒ-æ–‡æœ¬åŒ¹é…é¢„æµ‹

        Args:
            inputs: å›¾åƒè¾“å…¥ï¼Œå¯ä»¥æ˜¯æ–‡ä»¶è·¯å¾„ã€å­—èŠ‚æ•°æ®ã€æ–‡ä»¶å¯¹è±¡æˆ–PIL Image
            texts: æ–‡æœ¬åˆ—è¡¨
            **kwargs: å…¶ä»–é¢„æµ‹å‚æ•°
                - return_logits: æ˜¯å¦è¿”å›åŸå§‹logits
                - normalize_embeddings: æ˜¯å¦å½’ä¸€åŒ–åµŒå…¥å‘é‡

        Returns:
            Dict[str, Any]: é¢„æµ‹ç»“æœï¼ŒåŒ…å«ç›¸ä¼¼åº¦åˆ†æ•°ç­‰

        Raises:
            AIModelException: é¢„æµ‹å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        if self.status != ModelStatus.LOADED:
            raise AIModelException(
                f"æ¨¡å‹æœªåŠ è½½ï¼Œå½“å‰çŠ¶æ€: {get_enum_value(self.status)}",
                model_name=self.model_name
            )

        try:
            if not texts:
                raise AIModelException("æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º", model_name=self.model_name)

            # é¢„å¤„ç†å›¾åƒè¾“å…¥
            image = await self._preprocess_image(inputs)

            # è·å–é¢„æµ‹å‚æ•°
            return_logits = kwargs.get("return_logits", False)
            normalize_embeddings = kwargs.get("normalize_embeddings", self.config.get("normalize_embeddings", True))

            logger.info(f"å¼€å§‹å›¾åƒ-æ–‡æœ¬åŒ¹é…ï¼Œå›¾åƒ: {type(inputs)}, æ–‡æœ¬æ•°é‡: {len(texts)}")

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œé¢„æµ‹
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, self._match_sync, image, texts, return_logits, normalize_embeddings
            )

            self.record_usage()
            logger.info(f"å›¾åƒ-æ–‡æœ¬åŒ¹é…å®Œæˆï¼Œæœ€ä½³åŒ¹é…: {result['best_match']['text']}")
            return result

        except Exception as e:
            error_msg = f"å›¾åƒ-æ–‡æœ¬åŒ¹é…å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)

    async def _preprocess_image(self, image_input: Union[str, bytes, BinaryIO, Image.Image]) -> Image.Image:
        """
        é¢„å¤„ç†å›¾åƒè¾“å…¥

        Args:
            image_input: å›¾åƒè¾“å…¥

        Returns:
            Image.Image: å¤„ç†åçš„PILå›¾åƒ
        """
        # å¦‚æœå·²ç»æ˜¯PILå›¾åƒ
        if isinstance(image_input, Image.Image):
            return image_input.convert("RGB")

        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„
        elif isinstance(image_input, str):
            if image_input.startswith(("http://", "https://")):
                # ç½‘ç»œå›¾ç‰‡
                return await self._load_image_from_url(image_input)
            else:
                # æœ¬åœ°æ–‡ä»¶
                if not os.path.exists(image_input):
                    raise AIModelException(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_input}", model_name=self.model_name)

                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(image_input)
                if file_size > self.config.get("max_file_size", 10 * 1024 * 1024):
                    raise AIModelException(
                        f"å›¾åƒæ–‡ä»¶è¿‡å¤§ï¼Œå½“å‰å¤§å°: {file_size} bytesï¼Œæœ€å¤§é™åˆ¶: {self.config['max_file_size']} bytes",
                        model_name=self.model_name
                    )

                # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
                file_ext = Path(image_input).suffix.lower()
                if file_ext not in self.config.get("supported_formats", []):
                    raise AIModelException(
                        f"ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼: {file_ext}ï¼Œæ”¯æŒçš„æ ¼å¼: {self.config['supported_formats']}",
                        model_name=self.model_name
                    )

                return Image.open(image_input).convert("RGB")

        # å¦‚æœæ˜¯å­—èŠ‚æ•°æ®æˆ–æ–‡ä»¶å¯¹è±¡
        elif isinstance(image_input, (bytes, BinaryIO)):
            try:
                if isinstance(image_input, bytes):
                    image = Image.open(BytesIO(image_input))
                else:
                    image_input.seek(0)
                    image = Image.open(image_input)

                return image.convert("RGB")

            except Exception as e:
                raise AIModelException(f"æ‰“å¼€å›¾åƒæ•°æ®å¤±è´¥: {str(e)}", model_name=self.model_name)

        else:
            raise AIModelException(f"ä¸æ”¯æŒçš„å›¾åƒè¾“å…¥ç±»å‹: {type(image_input)}", model_name=self.model_name)

    async def _load_image_from_url(self, url: str) -> Image.Image:
        """
        ä»URLåŠ è½½å›¾åƒ

        Args:
            url: å›¾åƒURL

        Returns:
            Image.Image: PILå›¾åƒ
        """
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œç½‘ç»œè¯·æ±‚
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(None, requests.get, url, {"timeout": 10})

            response.raise_for_status()
            image = Image.open(BytesIO(response.content))
            return image.convert("RGB")

        except Exception as e:
            raise AIModelException(f"ä»URLåŠ è½½å›¾åƒå¤±è´¥: {str(e)}", model_name=self.model_name)

    def _match_sync(self, image: Image.Image, texts: List[str], return_logits: bool, normalize_embeddings: bool) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰§è¡Œå›¾åƒ-æ–‡æœ¬åŒ¹é…

        Args:
            image: PILå›¾åƒ
            texts: æ–‡æœ¬åˆ—è¡¨
            return_logits: æ˜¯å¦è¿”å›åŸå§‹logits
            normalize_embeddings: æ˜¯å¦å½’ä¸€åŒ–

        Returns:
            Dict[str, Any]: åŒ¹é…ç»“æœ
        """
        start_time = time.time()

        # å›¾åƒé¢„å¤„ç†
        max_size = self.config.get("max_image_size", 512)
        image = self._resize_image_keep_ratio(image, max_size)

        # é€ä¸ªå¤„ç†æ¯ä¸ªæ–‡æœ¬ï¼Œé¿å…ç»´åº¦é—®é¢˜
        similarities = []

        for text in texts:
            try:
                # ä½¿ç”¨Chinese-CLIPå¤„ç†å•ä¸ªå›¾åƒå’Œæ–‡æœ¬
                inputs = self.processor(
                    text=[text],  # åªå¤„ç†ä¸€ä¸ªæ–‡æœ¬
                    images=image,
                    padding=True,
                    return_tensors="pt"
                )
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

                with torch.no_grad():
                    outputs = self.model(**inputs)

                    # è·å–ç›¸ä¼¼åº¦åˆ†æ•°
                    if hasattr(outputs, 'logits_per_image') and outputs.logits_per_image is not None:
                        similarity = outputs.logits_per_image.cpu().numpy()
                    elif hasattr(outputs, 'logits_per_text') and outputs.logits_per_text is not None:
                        similarity = outputs.logits_per_text.T.cpu().numpy()
                    else:
                        # å¦‚æœæ²¡æœ‰logitsï¼Œä½¿ç”¨ç®€å•çš„é»˜è®¤ç›¸ä¼¼åº¦
                        similarity = np.array([[0.5]])  # é»˜è®¤ä¸­ç­‰ç›¸ä¼¼åº¦

                    # ç¡®ä¿similarityæ˜¯æ ‡é‡
                    if similarity.ndim >= 2:
                        similarity = similarity[0, 0]  # å–ç¬¬ä¸€ä¸ªå€¼
                    elif similarity.ndim == 1:
                        similarity = similarity[0]

                similarities.append(float(similarity))

            except Exception as e:
                logger.warning(f"å¤„ç†æ–‡æœ¬ '{text}' æ—¶å‡ºé”™: {str(e)}, ä½¿ç”¨é»˜è®¤ç›¸ä¼¼åº¦")
                similarities.append(0.0)  # é»˜è®¤ç›¸ä¼¼åº¦

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        similarities_array = np.array(similarities)  # [len(texts)]

        # è·å–æœ€ä½³åŒ¹é…
        best_idx = np.argmax(similarities_array)
        best_match = {
            "index": int(best_idx),
            "text": texts[best_idx],
            "similarity": float(similarities_array[best_idx])
        }

        # æ„å»ºç»“æœ
        result = {
            "best_match": best_match,
            "all_matches": [
                {
                    "index": i,
                    "text": texts[i],
                    "similarity": float(similarities_array[i]),
                    "logit": float(similarities_array[i]) if return_logits else None
                }
                for i in range(len(texts))
            ],
            "image_embedding": similarities_array.tolist(),
            "text_embeddings": np.eye(len(texts)).tolist(),  # å•ä½çŸ©é˜µä½œä¸ºå ä½ç¬¦
            "processing_time": time.time() - start_time,
            "model_name": self.model_name
        }

        return result

    def _resize_image_keep_ratio(self, image: Image.Image, max_size: int) -> Image.Image:
        """
        ä¿æŒé•¿å®½æ¯”è°ƒæ•´å›¾åƒå¤§å°

        Args:
            image: è¾“å…¥å›¾åƒ
            max_size: æœ€å¤§å°ºå¯¸

        Returns:
            Image.Image: è°ƒæ•´åçš„å›¾åƒ
        """
        # è½¬æ¢ä¸ºRGBï¼ˆå¤„ç†RGBAç­‰æ ¼å¼ï¼‰
        if image.mode != "RGB":
            image = image.convert("RGB")

        # è·å–åŸå§‹å°ºå¯¸
        width, height = image.size

        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        if max(width, height) > max_size:
            if width > height:
                new_width = max_size
                new_height = int(height * max_size / width)
            else:
                new_height = max_size
                new_width = int(width * max_size / height)

            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)

        return image

    async def encode_image(self, image_input: Union[str, bytes, BinaryIO, Image.Image]) -> np.ndarray:
        """
        ç¼–ç å›¾åƒä¸ºåµŒå…¥å‘é‡

        Args:
            image_input: å›¾åƒè¾“å…¥

        Returns:
            np.ndarray: å›¾åƒåµŒå…¥å‘é‡
        """
        if self.status != ModelStatus.LOADED:
            raise AIModelException(
                f"æ¨¡å‹æœªåŠ è½½ï¼Œå½“å‰çŠ¶æ€: {get_enum_value(self.status)}",
                model_name=self.model_name
            )

        try:
            # é¢„å¤„ç†å›¾åƒ
            image = await self._preprocess_image(image_input)

            # è°ƒæ•´å›¾åƒå¤§å°
            max_size = self.config.get("max_image_size", 512)
            image = self._resize_image_keep_ratio(image, max_size)

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œç¼–ç 
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(None, self._encode_image_sync, image)

            return embedding

        except Exception as e:
            error_msg = f"å›¾åƒç¼–ç å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)

    def _encode_image_sync(self, image: Image.Image) -> np.ndarray:
        """åŒæ­¥æ‰§è¡Œå›¾åƒç¼–ç """
        inputs = self.processor(images=image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model.get_image_features(**inputs)

        # å½’ä¸€åŒ–
        if self.config.get("normalize_embeddings", True):
            outputs = F.normalize(outputs, p=2, dim=-1)

        return outputs.cpu().numpy().flatten()

    async def encode_text(self, texts: List[str]) -> np.ndarray:
        """
        ç¼–ç æ–‡æœ¬ä¸ºåµŒå…¥å‘é‡

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            np.ndarray: æ–‡æœ¬åµŒå…¥å‘é‡çŸ©é˜µ
        """
        if self.status != ModelStatus.LOADED:
            raise AIModelException(
                f"æ¨¡å‹æœªåŠ è½½ï¼Œå½“å‰çŠ¶æ€: {get_enum_value(self.status)}",
                model_name=self.model_name
            )

        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œç¼–ç 
            loop = asyncio.get_event_loop()
            embeddings = await loop.run_in_executor(None, self._encode_text_sync, texts)

            return embeddings

        except Exception as e:
            error_msg = f"æ–‡æœ¬ç¼–ç å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)

    def _encode_text_sync(self, texts: List[str]) -> np.ndarray:
        """åŒæ­¥æ‰§è¡Œæ–‡æœ¬ç¼–ç """
        inputs = self.processor(text=texts, return_tensors="pt", padding=True, truncation=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model.get_text_features(**inputs)

        # å½’ä¸€åŒ–
        if self.config.get("normalize_embeddings", True):
            outputs = F.normalize(outputs, p=2, dim=-1)

        return outputs.cpu().numpy()

    async def search_images_by_text(self, query: str, image_embeddings: np.ndarray, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ–‡æœ¬æœç´¢ç›¸ä¼¼å›¾åƒ

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            image_embeddings: å›¾åƒåµŒå…¥çŸ©é˜µ [n_images, embed_dim]
            top_k: è¿”å›Top-Kç»“æœ

        Returns:
            List[Dict[str, Any]]: ç›¸ä¼¼å›¾åƒåˆ—è¡¨
        """
        try:
            # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
            text_embedding = await self.encode_text([query])
            text_embedding = text_embedding.flatten()

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = np.dot(image_embeddings, text_embedding)

            # è·å–Top-Kç»“æœ
            top_indices = np.argsort(similarities)[::-1][:top_k]

            results = []
            for idx in top_indices:
                results.append({
                    "index": int(idx),
                    "similarity": float(similarities[idx])
                })

            return results

        except Exception as e:
            error_msg = f"æ–‡æœ¬æœç´¢å›¾åƒå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)

    async def batch_match(self, image_inputs: List[Union[str, bytes, BinaryIO, Image.Image]], texts: List[str], **kwargs) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å›¾åƒ-æ–‡æœ¬åŒ¹é…

        Args:
            image_inputs: å›¾åƒè¾“å…¥åˆ—è¡¨
            texts: æ–‡æœ¬åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            List[Dict[str, Any]]: åŒ¹é…ç»“æœåˆ—è¡¨
        """
        results = []
        for i, image_input in enumerate(image_inputs):
            try:
                result = await self.predict(image_input, texts, **kwargs)
                result["batch_index"] = i
                results.append(result)
            except Exception as e:
                logger.error(f"æ‰¹é‡åŒ¹é…ç¬¬{i}ä¸ªå›¾åƒå¤±è´¥: {str(e)}")
                results.append({
                    "batch_index": i,
                    "error": str(e),
                    "success": False
                })

        return results

    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯

        Returns:
            Dict[str, Any]: æ¨¡å‹ä¿¡æ¯
        """
        return {
            "model_name": self.model_name,
            "model_type": get_enum_value(self.model_type),
            "provider": get_enum_value(self.provider),
            "device": str(self.device),
            "max_image_size": self.config.get("max_image_size", 512),
            "max_file_size": self.config.get("max_file_size", 10 * 1024 * 1024),
            "supported_formats": self.config.get("supported_formats", []),
            "normalize_embeddings": self.config.get("normalize_embeddings", True),
            "batch_size": self.config.get("batch_size", 16)
        }

    def _get_test_input(self) -> Tuple[str, List[str]]:
        """è·å–å¥åº·æ£€æŸ¥çš„æµ‹è¯•è¾“å…¥"""
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
        from PIL import Image, ImageDraw
        import numpy as np

        # åˆ›å»ºä¸€ä¸ª100x100çš„çº¢è‰²æ–¹å—
        image = Image.new('RGB', (100, 100), color='red')

        # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
        temp_file = os.path.join(tempfile.gettempdir(), "clip_test.jpg")
        image.save(temp_file)

        texts = ["çº¢è‰²", "è“è‰²", "ç»¿è‰²", "ä¸€ä¸ªçº¢è‰²çš„æ–¹å—"]
        return temp_file, texts

    async def benchmark_performance(self, test_images: List[Union[str, Image.Image]], test_texts: List[str], num_runs: int = 3) -> Dict[str, Any]:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•

        Args:
            test_images: æµ‹è¯•å›¾åƒåˆ—è¡¨
            test_texts: æµ‹è¯•æ–‡æœ¬åˆ—è¡¨
            num_runs: è¿è¡Œæ¬¡æ•°

        Returns:
            Dict[str, Any]: æ€§èƒ½æµ‹è¯•ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹CLIPæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œå›¾åƒæ•°é‡: {len(test_images)}, æ–‡æœ¬æ•°é‡: {len(test_texts)}, è¿è¡Œæ¬¡æ•°: {num_runs}")

            times = []
            for run in range(num_runs):
                start_time = time.time()
                await self.batch_match(test_images, test_texts)
                end_time = time.time()
                run_time = end_time - start_time
                times.append(run_time)
                logger.info(f"ç¬¬{run + 1}æ¬¡è¿è¡Œè€—æ—¶: {run_time:.3f}ç§’")

            avg_time = np.mean(times)
            throughput = (len(test_images) * len(test_texts)) / avg_time

            results = {
                "model_name": self.model_name,
                "num_images": len(test_images),
                "num_texts": len(test_texts),
                "total_comparisons": len(test_images) * len(test_texts),
                "num_runs": num_runs,
                "times": times,
                "avg_time": float(avg_time),
                "min_time": float(np.min(times)),
                "max_time": float(np.max(times)),
                "throughput": float(throughput),  # comparisons per second
                "device": str(self.device)
            }

            logger.info(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼Œå¹³å‡è€—æ—¶: {avg_time:.3f}ç§’ï¼Œååé‡: {throughput:.1f} comparisons/s")
            return results

        except Exception as e:
            error_msg = f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise AIModelException(error_msg, model_name=self.model_name)

    def is_ready(self) -> bool:
        """æ£€æŸ¥CLIPæ¨¡å‹æ˜¯å¦å·²åŠ è½½å¹¶å°±ç»ª"""
        return True


# åˆ›å»ºCLIPæœåŠ¡å®ä¾‹çš„å·¥å‚å‡½æ•°
def create_clip_service(config: Dict[str, Any] = None) -> CLIPVisionService:
    """
    åˆ›å»ºCLIPå›¾åƒç†è§£æœåŠ¡å®ä¾‹

    Args:
        config: æ¨¡å‹é…ç½®å‚æ•°

    Returns:
        CLIPVisionService: CLIPæœåŠ¡å®ä¾‹
    """
    return CLIPVisionService(config or {})


# å…¨å±€CLIPæœåŠ¡å®ä¾‹
_clip_service = None


def get_clip_service() -> CLIPVisionService:
    """
    è·å–CLIPå›¾åƒç†è§£æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    Returns:
        CLIPVisionService: CLIPæœåŠ¡å®ä¾‹
    """
    global _clip_service

    if _clip_service is None:
        _clip_service = create_clip_service()

    return _clip_service